# Промты и ответы

LLM_prompt и LMM_responce - содержат в себе полный текст статьи, из за чего точность предсказаний последних двух вопросов упала

LLM_prompt1 и LMM_responce1 - запрос и ответ с укороченным текстом статьи в следствии чего, ответ оказался более точным

# Ps

Загрузить модель при помощи кода не получилось, а в colab ответ генирировался очень долго,
поэтому я скачал самую простую модель с этого сайта https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct/tree/main.
Из-за того что модель очень простая, часто были неточности в ответах
